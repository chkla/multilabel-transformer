# Multilabel Classification with Huggingfaceâ€™ ğŸ¤— Trainer ğŸ’ª and AdapterHub ğŸ¤–: A short Tutorial for Multilabel Classification with Language Models
If you are a fan of the HuggingFace API ğŸ¤—, you may have noticed the new trainer ğŸ’ª class (introduced in version 2.9):

```python
trainer = Trainer(
    model,
    args,
    train_dataset=TRAIN_DATA,
    eval_dataset=TEST_DATA,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```


The trainer class provides an easy way to do the training process for tuning your own language model (e.g., BERT - if you have never heard of language models like BERT before, you should stop here first and look at this amazing blog post) in a few lines of code, with all the options to customize the training (check out an example provided by HuggingFace for sentence classification).

The official tutorial only provides you a way to use it without the new trainer ğŸ’ª class. I will show in this notebook a small example using AdapterHub ğŸ¤– doing the job for you by providing a multilabel head out-of-the box.


| type        | notebook           |
| ------------- |:-------------:|
| multilabel-adapter      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1xoL6ncoFGQnRHmFx-w9D_LAQMLEMHoa_?usp=sharing) |
| multilabel-transformer     |  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1aF4WeuNYDsIVWnp2xYMTgeRGQOAodO1g?usp=sharing)     |


You can also use the fast lane ğŸš€ by importing the `MultilabelTransformer` provided in this repository.
```python
from MultilabelTransformer import MultilabelRobertaForSequenceClassification

model = MultilabelRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=N)
```

_Note: `MultilabelTransformer` currently supports `MultilabelRobertaForSequenceClassification` and `MultilabelBertForSequenceClassification`._


Happy Researching ğŸ‘¨â€ğŸ”¬!
